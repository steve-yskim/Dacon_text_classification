{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_md_txt(klue_bert_base)md_img(NaN)b(128)L(5e-06)ml(128)d(0)a(0.1)an(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12405 data\n",
      "                 ID       label_0       label_1       label_2       label_3\n",
      "count  12405.000000  12405.000000  12405.000000  12405.000000  12405.000000\n",
      "mean    8290.676098      1.240871      0.080693      0.934220      0.918339\n",
      "std     4776.637412      0.715848      0.379266      0.945309      0.273858\n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%     4175.000000      1.000000      0.000000      0.000000      1.000000\n",
      "50%     8291.000000      1.000000      0.000000      1.000000      1.000000\n",
      "75%    12448.000000      1.000000      0.000000      2.000000      1.000000\n",
      "max    16540.000000      3.000000      2.000000      2.000000      1.000000\n",
      "found 4136 data\n",
      "                 ID      label_0      label_1      label_2      label_3\n",
      "count   4136.000000  4136.000000  4136.000000  4136.000000  4136.000000\n",
      "mean    8207.986702     1.240812     0.075435     0.915377     0.918762\n",
      "std     4770.604558     0.716054     0.366184     0.949815     0.273233\n",
      "min        4.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%     4012.750000     1.000000     0.000000     0.000000     1.000000\n",
      "50%     8195.000000     1.000000     0.000000     1.000000     1.000000\n",
      "75%    12296.500000     1.000000     0.000000     2.000000     1.000000\n",
      "max    16539.000000     3.000000     2.000000     2.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd28e987bf1948d78f193a9bf2ef117f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.6440884916785262 type_f1: 0.2395464932244433 polarity_f1: 0.32421311015378784 tense_f1: 0.3921261491499413 certainty_f1: 0.5017855824585216 whole_f1: 0.36315642392003805\n",
      "train_Loss: 0.6440884916785262 weighted_f1: 0.3644178337466735 whole_f1: 0.36315642392003805 type_f1: 0.2395464932244433 polarity_f: 0.32421311015378784 tense_f1: 0.3921261491499413 certainty_f1: 0.5017855824585216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21188116deac49ccb387aece02eda040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.46072056503314346 type_f1: 0.22521923996811055 polarity_f1: 0.32596070678364014 tense_f1: 0.5043216625085264 certainty_f1: 0.4788306451612903 whole_f1: 0.5015754043277555\n",
      "val_Loss: 0.46072056503314346 val_weighted_f1: 0.3835830636053918 val_whole_f1: 0.5015754043277555 val_type_f1: 0.22521923996811055 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.5043216625085264 val_certainty_f1: 0.4788306451612903\n",
      "val_f1 improved!! saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b9c1d87972471b955bae150eefec6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.41301283449090714 type_f1: 0.23436080914449053 polarity_f1: 0.3255091236610151 tense_f1: 0.5652092893547552 certainty_f1: 0.47867198991384746 whole_f1: 0.5492033362236725\n",
      "train_Loss: 0.41301283449090714 weighted_f1: 0.40093780301852705 whole_f1: 0.5492033362236725 type_f1: 0.23436080914449053 polarity_f: 0.3255091236610151 tense_f1: 0.5652092893547552 certainty_f1: 0.47867198991384746\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fef169dcaa44475b58bea7616427e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3216020191949387 type_f1: 0.2643853147029114 polarity_f1: 0.32596070678364014 tense_f1: 0.604069854020398 certainty_f1: 0.4788306451612903 whole_f1: 0.6085124842083571\n",
      "val_Loss: 0.3216020191949387 val_weighted_f1: 0.4183116301670599 val_whole_f1: 0.6085124842083571 val_type_f1: 0.2643853147029114 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.604069854020398 val_certainty_f1: 0.4788306451612903\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f6416935944192a240cea5b16652cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3250341906609049 type_f1: 0.3231063941671596 polarity_f1: 0.3255091236610151 tense_f1: 0.7163815311028351 certainty_f1: 0.520501360558994 whole_f1: 0.6134701237007648\n",
      "train_Loss: 0.3250341906609049 weighted_f1: 0.47137460237250095 whole_f1: 0.6134701237007648 type_f1: 0.3231063941671596 polarity_f: 0.3255091236610151 tense_f1: 0.7163815311028351 certainty_f1: 0.520501360558994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68742078ba5e4849ab952483e68bcfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.27092525530138145 type_f1: 0.3756544394928464 polarity_f1: 0.32596070678364014 tense_f1: 0.8189193093671259 certainty_f1: 0.6799742008932291 whole_f1: 0.6595280309252657\n",
      "val_Loss: 0.27092525530138145 val_weighted_f1: 0.5501271641342104 val_whole_f1: 0.6595280309252657 val_type_f1: 0.3756544394928464 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.8189193093671259 val_certainty_f1: 0.6799742008932291\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28d1c1e2ff147eaae225f1469db3496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.28690568490020696 type_f1: 0.43128567030074005 polarity_f1: 0.3270621981366728 tense_f1: 0.7994953533581226 certainty_f1: 0.6437043772732973 whole_f1: 0.6469089517970413\n",
      "train_Loss: 0.28690568490020696 weighted_f1: 0.5503868997672081 whole_f1: 0.6469089517970413 type_f1: 0.43128567030074005 polarity_f: 0.3270621981366728 tense_f1: 0.7994953533581226 certainty_f1: 0.6437043772732973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7257a14cd5340e5b45ca733c0514067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.24648964792720585 type_f1: 0.5267883652471501 polarity_f1: 0.32596070678364014 tense_f1: 0.8401479306010463 certainty_f1: 0.7379443874207376 whole_f1: 0.6826891349917734\n",
      "val_Loss: 0.24648964792720585 val_weighted_f1: 0.6077103475131436 val_whole_f1: 0.6826891349917734 val_type_f1: 0.5267883652471501 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.8401479306010463 val_certainty_f1: 0.7379443874207376\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00f84230f95413e9fc2d5374695989a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.25963208766076795 type_f1: 0.5609632514994216 polarity_f1: 0.35212642056797067 tense_f1: 0.8203666407261708 certainty_f1: 0.6932988995576653 whole_f1: 0.6728968618624117\n",
      "train_Loss: 0.25963208766076795 weighted_f1: 0.606688803087807 whole_f1: 0.6728968618624117 type_f1: 0.5609632514994216 polarity_f: 0.35212642056797067 tense_f1: 0.8203666407261708 certainty_f1: 0.6932988995576653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0623353af449b1842bb429415ab8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22248703886969176 type_f1: 0.6920798808887394 polarity_f1: 0.4121119689892924 tense_f1: 0.8482216145143618 certainty_f1: 0.7577405762383421 whole_f1: 0.7111996418383838\n",
      "val_Loss: 0.22248703886969176 val_weighted_f1: 0.677538510157684 val_whole_f1: 0.7111996418383838 val_type_f1: 0.6920798808887394 val_polarity_f: 0.4121119689892924 val_tense_f1: 0.8482216145143618 val_certainty_f1: 0.7577405762383421\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee393c7a0d455dbd6aed566aaff5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2380944700622597 type_f1: 0.6225639928638702 polarity_f1: 0.4856732438445026 tense_f1: 0.8359344996175797 certainty_f1: 0.7070142286610951 whole_f1: 0.697340292769155\n",
      "train_Loss: 0.2380944700622597 weighted_f1: 0.6627964912467619 whole_f1: 0.697340292769155 type_f1: 0.6225639928638702 polarity_f: 0.4856732438445026 tense_f1: 0.8359344996175797 certainty_f1: 0.7070142286610951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d5b346a6c04291b8ae79c61c9bdd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2094970917332795 type_f1: 0.7625100203606232 polarity_f1: 0.5850765810937412 tense_f1: 0.8537908237908237 certainty_f1: 0.7690050339186949 whole_f1: 0.7420649601509507\n",
      "val_Loss: 0.2094970917332795 val_weighted_f1: 0.7425956147909707 val_whole_f1: 0.7420649601509507 val_type_f1: 0.7625100203606232 val_polarity_f: 0.5850765810937412 val_tense_f1: 0.8537908237908237 val_certainty_f1: 0.7690050339186949\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fca5b7a987f469e9c267205d52116b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.225150570395924 type_f1: 0.6769160198350606 polarity_f1: 0.5842530818438899 tense_f1: 0.8392352188386546 certainty_f1: 0.7167952457637388 whole_f1: 0.7141239647515241\n",
      "train_Loss: 0.225150570395924 weighted_f1: 0.704299891570336 whole_f1: 0.7141239647515241 type_f1: 0.6769160198350606 polarity_f: 0.5842530818438899 tense_f1: 0.8392352188386546 certainty_f1: 0.7167952457637388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d948c02c0f4950b1461cbf9ad8e0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20101645469262014 type_f1: 0.7677327493058341 polarity_f1: 0.6283986084554596 tense_f1: 0.8549839291481725 certainty_f1: 0.7292338814461223 whole_f1: 0.740042838620777\n",
      "val_Loss: 0.20101645469262014 val_weighted_f1: 0.7450872920888971 val_whole_f1: 0.740042838620777 val_type_f1: 0.7677327493058341 val_polarity_f: 0.6283986084554596 val_tense_f1: 0.8549839291481725 val_certainty_f1: 0.7292338814461223\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736ffa559918494fad2bd9af002462df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2146780713677935 type_f1: 0.7119420375012266 polarity_f1: 0.6498950688691921 tense_f1: 0.8464281606569711 certainty_f1: 0.7314352334623141 whole_f1: 0.7287017028993017\n",
      "train_Loss: 0.2146780713677935 weighted_f1: 0.734925125122426 whole_f1: 0.7287017028993017 type_f1: 0.7119420375012266 polarity_f: 0.6498950688691921 tense_f1: 0.8464281606569711 certainty_f1: 0.7314352334623141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be28bd26349749c491213e09fd963838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19682698715586266 type_f1: 0.7750849398594339 polarity_f1: 0.6238501676920736 tense_f1: 0.8591427014091448 certainty_f1: 0.7467435646864109 whole_f1: 0.7457785524453621\n",
      "val_Loss: 0.19682698715586266 val_weighted_f1: 0.7512053434117658 val_whole_f1: 0.7457785524453621 val_type_f1: 0.7750849398594339 val_polarity_f: 0.6238501676920736 val_tense_f1: 0.8591427014091448 val_certainty_f1: 0.7467435646864109\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b4cf822706465a8124dc3ebbed0986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20563046055328266 type_f1: 0.7420171954813777 polarity_f1: 0.6469171376256887 tense_f1: 0.8526524512092312 certainty_f1: 0.7429194674696455 whole_f1: 0.7401957812225011\n",
      "train_Loss: 0.20563046055328266 weighted_f1: 0.7461265629464857 whole_f1: 0.7401957812225011 type_f1: 0.7420171954813777 polarity_f: 0.6469171376256887 tense_f1: 0.8526524512092312 certainty_f1: 0.7429194674696455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c57bb8ef77242e3b5965840f0355ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19691544451266238 type_f1: 0.8119181071409461 polarity_f1: 0.6398256522694404 tense_f1: 0.8584992548137582 certainty_f1: 0.7770581456277333 whole_f1: 0.7492447245347087\n",
      "val_Loss: 0.19691544451266238 val_weighted_f1: 0.7718252899629695 val_whole_f1: 0.7492447245347087 val_type_f1: 0.8119181071409461 val_polarity_f: 0.6398256522694404 val_tense_f1: 0.8584992548137582 val_certainty_f1: 0.7770581456277333\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1459d83d4b4576ac3be7111bda51d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1981633402223983 type_f1: 0.7591346223836026 polarity_f1: 0.6839726497888541 tense_f1: 0.8644539944726081 certainty_f1: 0.7399809445577078 whole_f1: 0.7456387680968384\n",
      "train_Loss: 0.1981633402223983 weighted_f1: 0.7618855528006931 whole_f1: 0.7456387680968384 type_f1: 0.7591346223836026 polarity_f: 0.6839726497888541 tense_f1: 0.8644539944726081 certainty_f1: 0.7399809445577078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10877d02482941148faa8fb6a263931b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19294329005612396 type_f1: 0.7905914905982325 polarity_f1: 0.6627368470046628 tense_f1: 0.8631719224015115 certainty_f1: 0.7647881231114084 whole_f1: 0.7509619680973624\n",
      "val_Loss: 0.19294329005612396 val_weighted_f1: 0.7703220957789538 val_whole_f1: 0.7509619680973624 val_type_f1: 0.7905914905982325 val_polarity_f: 0.6627368470046628 val_tense_f1: 0.8631719224015115 val_certainty_f1: 0.7647881231114084\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5433a07a30d743beaf754ee9471afa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1941367362137525 type_f1: 0.7615301224704799 polarity_f1: 0.6963755845447755 tense_f1: 0.8614673733643675 certainty_f1: 0.7377058629465958 whole_f1: 0.749065439683961\n",
      "train_Loss: 0.1941367362137525 weighted_f1: 0.7642697358315547 whole_f1: 0.749065439683961 type_f1: 0.7615301224704799 polarity_f: 0.6963755845447755 tense_f1: 0.8614673733643675 certainty_f1: 0.7377058629465958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3b9e335e3141369a8d3dc6bf16f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19224082135254916 type_f1: 0.7924412654194755 polarity_f1: 0.6467603284506754 tense_f1: 0.8634352424204726 certainty_f1: 0.7620632758138283 whole_f1: 0.7478388269874527\n",
      "val_Loss: 0.19224082135254916 val_weighted_f1: 0.7661750280261129 val_whole_f1: 0.7478388269874527 val_type_f1: 0.7924412654194755 val_polarity_f: 0.6467603284506754 val_tense_f1: 0.8634352424204726 val_certainty_f1: 0.7620632758138283\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6abd4bd41ec44adb0b644c704b0816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1851435078687122 type_f1: 0.7746574548925207 polarity_f1: 0.7065819545950296 tense_f1: 0.8689675298748175 certainty_f1: 0.752271489411094 whole_f1: 0.7566821964482101\n",
      "train_Loss: 0.1851435078687122 weighted_f1: 0.7756196071933654 whole_f1: 0.7566821964482101 type_f1: 0.7746574548925207 polarity_f: 0.7065819545950296 tense_f1: 0.8689675298748175 certainty_f1: 0.752271489411094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902a3e202834470b17d07c317342b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19167130872088428 type_f1: 0.801655991741027 polarity_f1: 0.6617514753622927 tense_f1: 0.8641610917117776 certainty_f1: 0.7788947023232913 whole_f1: 0.7547010277017883\n",
      "val_Loss: 0.19167130872088428 val_weighted_f1: 0.7766158152845971 val_whole_f1: 0.7547010277017883 val_type_f1: 0.801655991741027 val_polarity_f: 0.6617514753622927 val_tense_f1: 0.8641610917117776 val_certainty_f1: 0.7788947023232913\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd842ea0441840c29dd94eabcc694191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1804190223013283 type_f1: 0.7862299684551673 polarity_f1: 0.6969999252789766 tense_f1: 0.8769160816724653 certainty_f1: 0.767346274920871 whole_f1: 0.7655394482911161\n",
      "train_Loss: 0.1804190223013283 weighted_f1: 0.78187306258187 whole_f1: 0.7655394482911161 type_f1: 0.7862299684551673 polarity_f: 0.6969999252789766 tense_f1: 0.8769160816724653 certainty_f1: 0.767346274920871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6aa32162b8a4c939c83e6c2171f61ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19119619274047173 type_f1: 0.7837206794345107 polarity_f1: 0.6439082590309205 tense_f1: 0.8592309309818894 certainty_f1: 0.7562783763156494 whole_f1: 0.750021103336594\n",
      "val_Loss: 0.19119619274047173 val_weighted_f1: 0.7607845614407425 val_whole_f1: 0.750021103336594 val_type_f1: 0.7837206794345107 val_polarity_f: 0.6439082590309205 val_tense_f1: 0.8592309309818894 val_certainty_f1: 0.7562783763156494\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223048eb1a364f2bae43fd2852c04d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17403767231802267 type_f1: 0.790762887023444 polarity_f1: 0.7100439525616777 tense_f1: 0.8845477303719641 certainty_f1: 0.7542917466214945 whole_f1: 0.7719538747462524\n",
      "train_Loss: 0.17403767231802267 weighted_f1: 0.784911579144645 whole_f1: 0.7719538747462524 type_f1: 0.790762887023444 polarity_f: 0.7100439525616777 tense_f1: 0.8845477303719641 certainty_f1: 0.7542917466214945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769dc3ae37649d2b636fd8a8469b15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.18991986117406787 type_f1: 0.7890053191187159 polarity_f1: 0.6772349536480617 tense_f1: 0.8673207256116336 certainty_f1: 0.7610308281617699 whole_f1: 0.7560369439820951\n",
      "val_Loss: 0.18991986117406787 val_weighted_f1: 0.7736479566350454 val_whole_f1: 0.7560369439820951 val_type_f1: 0.7890053191187159 val_polarity_f: 0.6772349536480617 val_tense_f1: 0.8673207256116336 val_certainty_f1: 0.7610308281617699\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a12f549d9b140e9883a4d9218863b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1692140472519066 type_f1: 0.7986662888000428 polarity_f1: 0.7348317466253743 tense_f1: 0.885409034885039 certainty_f1: 0.7691202013251219 whole_f1: 0.7789162344180364\n",
      "train_Loss: 0.1692140472519066 weighted_f1: 0.7970068179088945 whole_f1: 0.7789162344180364 type_f1: 0.7986662888000428 polarity_f: 0.7348317466253743 tense_f1: 0.885409034885039 certainty_f1: 0.7691202013251219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932cc2932e9a4514a198dba62f5a473d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19318469561189702 type_f1: 0.799919542963607 polarity_f1: 0.6643618203544016 tense_f1: 0.8601286750220082 certainty_f1: 0.767664558271217 whole_f1: 0.7552843553992811\n",
      "val_Loss: 0.19318469561189702 val_weighted_f1: 0.7730186491528085 val_whole_f1: 0.7552843553992811 val_type_f1: 0.799919542963607 val_polarity_f: 0.6643618203544016 val_tense_f1: 0.8601286750220082 val_certainty_f1: 0.767664558271217\n",
      "val_accuracy is not improved...\n",
      "found 7090 data\n",
      "                ID  label_0  label_1  label_2  label_3\n",
      "count  7090.000000   7090.0   7090.0   7090.0   7090.0\n",
      "mean    493.724260      1.0      0.0      2.0      1.0\n",
      "std     291.359857      0.0      0.0      0.0      0.0\n",
      "min       0.000000      1.0      0.0      2.0      1.0\n",
      "25%     240.000000      1.0      0.0      2.0      1.0\n",
      "50%     493.000000      1.0      0.0      2.0      1.0\n",
      "75%     746.000000      1.0      0.0      2.0      1.0\n",
      "max     999.000000      1.0      0.0      2.0      1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1241cfbda2894dec9fd749ef9671916e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_md_txt(klue_bert_base)md_img(NaN)b(128)L(5e-06)ml(128)d(0)a(0.1)an(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12406 data\n",
      "                 ID       label_0       label_1       label_2       label_3\n",
      "count  12406.000000  12406.000000  12406.000000  12406.000000  12406.000000\n",
      "mean    8260.723279      1.240851      0.077221      0.929711      0.918427\n",
      "std     4776.931165      0.716047      0.371064      0.946140      0.273725\n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%     4117.750000      1.000000      0.000000      0.000000      1.000000\n",
      "50%     8261.000000      1.000000      0.000000      1.000000      1.000000\n",
      "75%    12418.750000      1.000000      0.000000      2.000000      1.000000\n",
      "max    16540.000000      3.000000      2.000000      2.000000      1.000000\n",
      "found 4135 data\n",
      "                 ID      label_0      label_1      label_2      label_3\n",
      "count   4135.000000  4135.000000  4135.000000  4135.000000  4135.000000\n",
      "mean    8297.832406     1.240871     0.085852     0.928900     0.918501\n",
      "std     4770.149890     0.715455     0.390540     0.947471     0.273634\n",
      "min        2.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%     4195.500000     1.000000     0.000000     0.000000     1.000000\n",
      "50%     8289.000000     1.000000     0.000000     1.000000     1.000000\n",
      "75%    12363.000000     1.000000     0.000000     2.000000     1.000000\n",
      "max    16538.000000     3.000000     2.000000     2.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b372edf0d4dc43f3a6297ff45d184b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.6598482622593694 type_f1: 0.24386701571764632 polarity_f1: 0.3117206277007047 tense_f1: 0.39935717302467905 certainty_f1: 0.509066135857268 whole_f1: 0.37543616430646604\n",
      "train_Loss: 0.6598482622593694 weighted_f1: 0.36600273807507455 whole_f1: 0.37543616430646604 type_f1: 0.24386701571764632 polarity_f: 0.3117206277007047 tense_f1: 0.39935717302467905 certainty_f1: 0.509066135857268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec6a2c62cc44ade85e78569e506ec8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.46613571736803944 type_f1: 0.22524916943521595 polarity_f1: 0.32498760535448684 tense_f1: 0.5131303620004185 certainty_f1: 0.47875961174839277 whole_f1: 0.4935833986356104\n",
      "val_Loss: 0.46613571736803944 val_weighted_f1: 0.38553168713462854 val_whole_f1: 0.4935833986356104 val_type_f1: 0.22524916943521595 val_polarity_f: 0.32498760535448684 val_tense_f1: 0.5131303620004185 val_certainty_f1: 0.47875961174839277\n",
      "val_f1 improved!! saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8ce806954b484c9596d621fff5d719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.41501048213528874 type_f1: 0.2443502992156564 polarity_f1: 0.32583312728371655 tense_f1: 0.5701181861870511 certainty_f1: 0.4787394957983193 whole_f1: 0.5492361839554032\n",
      "train_Loss: 0.41501048213528874 weighted_f1: 0.40476027712118584 whole_f1: 0.5492361839554032 type_f1: 0.2443502992156564 polarity_f: 0.32583312728371655 tense_f1: 0.5701181861870511 certainty_f1: 0.4787394957983193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ded33881c24f7f9e79518ad6ccbb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3283860990617699 type_f1: 0.27999152426870194 polarity_f1: 0.32498760535448684 tense_f1: 0.6632992885969734 certainty_f1: 0.47869389813414015 whole_f1: 0.608589542242668\n",
      "val_Loss: 0.3283860990617699 val_weighted_f1: 0.43674307908857557 val_whole_f1: 0.608589542242668 val_type_f1: 0.27999152426870194 val_polarity_f: 0.32498760535448684 val_tense_f1: 0.6632992885969734 val_certainty_f1: 0.47869389813414015\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8646c6bf05514285835c539a9e28f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.31736528004789744 type_f1: 0.3375377081287455 polarity_f1: 0.32583312728371655 tense_f1: 0.7396449199154497 certainty_f1: 0.5247492898350253 whole_f1: 0.6238298793326011\n",
      "train_Loss: 0.31736528004789744 weighted_f1: 0.4819412612907342 whole_f1: 0.6238298793326011 type_f1: 0.3375377081287455 polarity_f: 0.32583312728371655 tense_f1: 0.7396449199154497 certainty_f1: 0.5247492898350253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65f6c17e37b41f283c0dfce278a136c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2724472121791655 type_f1: 0.410434351868711 polarity_f1: 0.32498760535448684 tense_f1: 0.8265711413391643 certainty_f1: 0.6647583054251093 whole_f1: 0.6554361463083075\n",
      "val_Loss: 0.2724472121791655 val_weighted_f1: 0.5566878509968679 val_whole_f1: 0.6554361463083075 val_type_f1: 0.410434351868711 val_polarity_f: 0.32498760535448684 val_tense_f1: 0.8265711413391643 val_certainty_f1: 0.6647583054251093\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06042908217484e9e92f560e7283083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2719511815891638 type_f1: 0.5068155200973292 polarity_f1: 0.3617288681878034 tense_f1: 0.8142771186350068 certainty_f1: 0.6521821657488932 whole_f1: 0.6613715531924756\n",
      "train_Loss: 0.2719511815891638 weighted_f1: 0.5837509181672582 whole_f1: 0.6613715531924756 type_f1: 0.5068155200973292 polarity_f: 0.3617288681878034 tense_f1: 0.8142771186350068 certainty_f1: 0.6521821657488932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9e8f3c358c4ddeb62a47be0b1b889b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.24120896829082716 type_f1: 0.6085981514995789 polarity_f1: 0.43031735055967246 tense_f1: 0.8220556882753035 certainty_f1: 0.7168722841527375 whole_f1: 0.687771476413176\n",
      "val_Loss: 0.24120896829082716 val_weighted_f1: 0.6444608686218232 val_whole_f1: 0.687771476413176 val_type_f1: 0.6085981514995789 val_polarity_f: 0.43031735055967246 val_tense_f1: 0.8220556882753035 val_certainty_f1: 0.7168722841527375\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8354ca8df1b9410588f5606f23350529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.243574952444549 type_f1: 0.6507176299093166 polarity_f1: 0.5083842196234155 tense_f1: 0.826936713408044 certainty_f1: 0.6938225155612722 whole_f1: 0.6953737281757068\n",
      "train_Loss: 0.243574952444549 weighted_f1: 0.6699652696255121 whole_f1: 0.6953737281757068 type_f1: 0.6507176299093166 polarity_f: 0.5083842196234155 tense_f1: 0.826936713408044 certainty_f1: 0.6938225155612722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b073702704c34839a36bcd41c94a63d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22776084145150421 type_f1: 0.6897554102292509 polarity_f1: 0.5829027918031892 tense_f1: 0.8372347925791469 certainty_f1: 0.7374142384500906 whole_f1: 0.7197442955155117\n",
      "val_Loss: 0.22776084145150421 val_weighted_f1: 0.7118268082654193 val_whole_f1: 0.7197442955155117 val_type_f1: 0.6897554102292509 val_polarity_f: 0.5829027918031892 val_tense_f1: 0.8372347925791469 val_certainty_f1: 0.7374142384500906\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da23208a675408abe9158f02a47b205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22474529916444902 type_f1: 0.7012148575140619 polarity_f1: 0.5917695858108237 tense_f1: 0.8474864432962924 certainty_f1: 0.702878866567306 whole_f1: 0.7219886104330415\n",
      "train_Loss: 0.22474529916444902 weighted_f1: 0.710837438297121 whole_f1: 0.7219886104330415 type_f1: 0.7012148575140619 polarity_f: 0.5917695858108237 tense_f1: 0.8474864432962924 certainty_f1: 0.702878866567306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e151269666e34b40a8a2065b636f997c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2197881158995369 type_f1: 0.7399510802064332 polarity_f1: 0.6448582587578123 tense_f1: 0.8369072757563266 certainty_f1: 0.764587136027814 whole_f1: 0.7260900910913302\n",
      "val_Loss: 0.2197881158995369 val_weighted_f1: 0.7465759376870965 val_whole_f1: 0.7260900910913302 val_type_f1: 0.7399510802064332 val_polarity_f: 0.6448582587578123 val_tense_f1: 0.8369072757563266 val_certainty_f1: 0.764587136027814\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64294c94f5f34e3f94dff2690236141a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21640457772649374 type_f1: 0.7481359616150104 polarity_f1: 0.6165741431522395 tense_f1: 0.8452894369357122 certainty_f1: 0.726117175260688 whole_f1: 0.7278617677777386\n",
      "train_Loss: 0.21640457772649374 weighted_f1: 0.7340291792409126 whole_f1: 0.7278617677777386 type_f1: 0.7481359616150104 polarity_f: 0.6165741431522395 tense_f1: 0.8452894369357122 certainty_f1: 0.726117175260688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eb169095be4a5394fc756a57b8428a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21716739265056734 type_f1: 0.7222413416607004 polarity_f1: 0.6357589625258973 tense_f1: 0.8423673339504835 certainty_f1: 0.7596200710460336 whole_f1: 0.7204468417846122\n",
      "val_Loss: 0.21716739265056734 val_weighted_f1: 0.7399969272957787 val_whole_f1: 0.7204468417846122 val_type_f1: 0.7222413416607004 val_polarity_f: 0.6357589625258973 val_tense_f1: 0.8423673339504835 val_certainty_f1: 0.7596200710460336\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9526073776349a1b974ff0c11ddbebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20606435328726805 type_f1: 0.7476142051789958 polarity_f1: 0.647155154668102 tense_f1: 0.852882170088168 certainty_f1: 0.7355707898093998 whole_f1: 0.7365953912637863\n",
      "train_Loss: 0.20606435328726805 weighted_f1: 0.7458055799361664 whole_f1: 0.7365953912637863 type_f1: 0.7476142051789958 polarity_f: 0.647155154668102 tense_f1: 0.852882170088168 certainty_f1: 0.7355707898093998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1816b0103c29419e9e66cbb59988712e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21600401604045835 type_f1: 0.7282983110591299 polarity_f1: 0.6952110887244639 tense_f1: 0.8441052441261009 certainty_f1: 0.7459210440095766 whole_f1: 0.7230840000803621\n",
      "val_Loss: 0.21600401604045835 val_weighted_f1: 0.7533839219798178 val_whole_f1: 0.7230840000803621 val_type_f1: 0.7282983110591299 val_polarity_f: 0.6952110887244639 val_tense_f1: 0.8441052441261009 val_certainty_f1: 0.7459210440095766\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5a09b51d6d43dd94d1956bdda09707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1995618646421222 type_f1: 0.7633797510915874 polarity_f1: 0.6594742546676455 tense_f1: 0.8553084422641772 certainty_f1: 0.7412671300471698 whole_f1: 0.7445753282915618\n",
      "train_Loss: 0.1995618646421222 weighted_f1: 0.7548573945176451 whole_f1: 0.7445753282915618 type_f1: 0.7633797510915874 polarity_f: 0.6594742546676455 tense_f1: 0.8553084422641772 certainty_f1: 0.7412671300471698\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff7ecf33a764bdab5f1c799a37ff624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.213674490721367 type_f1: 0.7318717747411233 polarity_f1: 0.7046851482716203 tense_f1: 0.8514834673274763 certainty_f1: 0.7507378927507867 whole_f1: 0.7251417140877237\n",
      "val_Loss: 0.213674490721367 val_weighted_f1: 0.7596945707727516 val_whole_f1: 0.7251417140877237 val_type_f1: 0.7318717747411233 val_polarity_f: 0.7046851482716203 val_tense_f1: 0.8514834673274763 val_certainty_f1: 0.7507378927507867\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe543d806794068b416b310185d1193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19074356292171593 type_f1: 0.7730902004492772 polarity_f1: 0.6888750259632589 tense_f1: 0.8633199417934065 certainty_f1: 0.7468776516974168 whole_f1: 0.7513620385995446\n",
      "train_Loss: 0.19074356292171593 weighted_f1: 0.7680407049758399 whole_f1: 0.7513620385995446 type_f1: 0.7730902004492772 polarity_f: 0.6888750259632589 tense_f1: 0.8633199417934065 certainty_f1: 0.7468776516974168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19344aba7b344a03a947c198a3e2cc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2119300771725365 type_f1: 0.7402050625503372 polarity_f1: 0.6973882475556223 tense_f1: 0.8555530118396552 certainty_f1: 0.7386058323514464 whole_f1: 0.7380217001278983\n",
      "val_Loss: 0.2119300771725365 val_weighted_f1: 0.7579380385742653 val_whole_f1: 0.7380217001278983 val_type_f1: 0.7402050625503372 val_polarity_f: 0.6973882475556223 val_tense_f1: 0.8555530118396552 val_certainty_f1: 0.7386058323514464\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2773f32aa5b449cbbd8a7873501caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.18347112186086115 type_f1: 0.7820462153212748 polarity_f1: 0.6867023240204716 tense_f1: 0.8715010872998518 certainty_f1: 0.7522711780769213 whole_f1: 0.7618550416916737\n",
      "train_Loss: 0.18347112186086115 weighted_f1: 0.7731302011796299 whole_f1: 0.7618550416916737 type_f1: 0.7820462153212748 polarity_f: 0.6867023240204716 tense_f1: 0.8715010872998518 certainty_f1: 0.7522711780769213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb2162a46194b6a8f17a0d6e7bea1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21034806816869087 type_f1: 0.7464585789321511 polarity_f1: 0.6904500866632816 tense_f1: 0.8490406265229654 certainty_f1: 0.7628343962676025 whole_f1: 0.7345764009793458\n",
      "val_Loss: 0.21034806816869087 val_weighted_f1: 0.7621959220965001 val_whole_f1: 0.7345764009793458 val_type_f1: 0.7464585789321511 val_polarity_f: 0.6904500866632816 val_tense_f1: 0.8490406265229654 val_certainty_f1: 0.7628343962676025\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90633a7863d4ea9a50de65b81804f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17993685902462223 type_f1: 0.8041064082931492 polarity_f1: 0.6852655011264153 tense_f1: 0.8755001206736374 certainty_f1: 0.7559751262655254 whole_f1: 0.7673726937751386\n",
      "train_Loss: 0.17993685902462223 weighted_f1: 0.7802117890896818 whole_f1: 0.7673726937751386 type_f1: 0.8041064082931492 polarity_f: 0.6852655011264153 tense_f1: 0.8755001206736374 certainty_f1: 0.7559751262655254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1353d789f4e4680a13ff2d18413a83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2104008340417546 type_f1: 0.7423371287445988 polarity_f1: 0.7015784141204074 tense_f1: 0.8497090674236403 certainty_f1: 0.7441813007514493 whole_f1: 0.733487767195828\n",
      "val_Loss: 0.2104008340417546 val_weighted_f1: 0.759451477760024 val_whole_f1: 0.733487767195828 val_type_f1: 0.7423371287445988 val_polarity_f: 0.7015784141204074 val_tense_f1: 0.8497090674236403 val_certainty_f1: 0.7441813007514493\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fbbd8a96ce4a1aa4268d0c1292cf7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17484940930314552 type_f1: 0.8145822784257735 polarity_f1: 0.6877428717448014 tense_f1: 0.8762707517815146 certainty_f1: 0.7594321413271646 whole_f1: 0.7704313123457995\n",
      "train_Loss: 0.17484940930314552 weighted_f1: 0.7845070108198136 whole_f1: 0.7704313123457995 type_f1: 0.8145822784257735 polarity_f: 0.6877428717448014 tense_f1: 0.8762707517815146 certainty_f1: 0.7594321413271646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34752b9c761433c8d0ac1d669d149e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2099573120240577 type_f1: 0.7667444620114465 polarity_f1: 0.7069267428338416 tense_f1: 0.8545810154645364 certainty_f1: 0.7559641025448326 whole_f1: 0.7367971702441481\n",
      "val_Loss: 0.2099573120240577 val_weighted_f1: 0.7710540807136643 val_whole_f1: 0.7367971702441481 val_type_f1: 0.7667444620114465 val_polarity_f: 0.7069267428338416 val_tense_f1: 0.8545810154645364 val_certainty_f1: 0.7559641025448326\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccadc75d66f147faad65de8d73c32e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1660464216187637 type_f1: 0.8161800134900808 polarity_f1: 0.711903128789377 tense_f1: 0.8837304087110333 certainty_f1: 0.7822070632288008 whole_f1: 0.7840292968353291\n",
      "train_Loss: 0.1660464216187637 weighted_f1: 0.798505153554823 whole_f1: 0.7840292968353291 type_f1: 0.8161800134900808 polarity_f: 0.711903128789377 tense_f1: 0.8837304087110333 certainty_f1: 0.7822070632288008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce177d81ab6418aba0e6ffcd39bbbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21371334427933883 type_f1: 0.7326888381380254 polarity_f1: 0.7090842569038956 tense_f1: 0.8569260061035212 certainty_f1: 0.7429015544041451 whole_f1: 0.7341764191436243\n",
      "val_Loss: 0.21371334427933883 val_weighted_f1: 0.7604001638873968 val_whole_f1: 0.7341764191436243 val_type_f1: 0.7326888381380254 val_polarity_f: 0.7090842569038956 val_tense_f1: 0.8569260061035212 val_certainty_f1: 0.7429015544041451\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e204a2fd14a4ac1830ca5e6e0c3fef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.16087212137170478 type_f1: 0.8261068640456366 polarity_f1: 0.7303126819125151 tense_f1: 0.8853751206737194 certainty_f1: 0.7688699529656099 whole_f1: 0.788803123905561\n",
      "train_Loss: 0.16087212137170478 weighted_f1: 0.8026661548993702 whole_f1: 0.788803123905561 type_f1: 0.8261068640456366 polarity_f: 0.7303126819125151 tense_f1: 0.8853751206737194 certainty_f1: 0.7688699529656099\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b71e338a74bb790a6550db6822962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21410480105603044 type_f1: 0.7423286759708558 polarity_f1: 0.7241898206547891 tense_f1: 0.8481570686309831 certainty_f1: 0.7302707852741426 whole_f1: 0.73084828663276\n",
      "val_Loss: 0.21410480105603044 val_weighted_f1: 0.7612365876326926 val_whole_f1: 0.73084828663276 val_type_f1: 0.7423286759708558 val_polarity_f: 0.7241898206547891 val_tense_f1: 0.8481570686309831 val_certainty_f1: 0.7302707852741426\n",
      "val_accuracy is not improved...\n",
      "found 7090 data\n",
      "                ID  label_0  label_1  label_2  label_3\n",
      "count  7090.000000   7090.0   7090.0   7090.0   7090.0\n",
      "mean    493.724260      1.0      0.0      2.0      1.0\n",
      "std     291.359857      0.0      0.0      0.0      0.0\n",
      "min       0.000000      1.0      0.0      2.0      1.0\n",
      "25%     240.000000      1.0      0.0      2.0      1.0\n",
      "50%     493.000000      1.0      0.0      2.0      1.0\n",
      "75%     746.000000      1.0      0.0      2.0      1.0\n",
      "max     999.000000      1.0      0.0      2.0      1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ebcd09b8704383a1165b2f910de45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_md_txt(klue_bert_base)md_img(NaN)b(128)L(5e-06)ml(128)d(0)a(0.1)an(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12406 data\n",
      "                 ID       label_0       label_1       label_2       label_3\n",
      "count  12406.000000  12406.000000  12406.000000  12406.000000  12406.000000\n",
      "mean    8282.584959      1.240851      0.081412      0.930034      0.917137\n",
      "std     4775.140271      0.715822      0.380052      0.947355      0.275686\n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%     4154.250000      1.000000      0.000000      0.000000      1.000000\n",
      "50%     8256.500000      1.000000      0.000000      1.000000      1.000000\n",
      "75%    12415.750000      1.000000      0.000000      2.000000      1.000000\n",
      "max    16539.000000      3.000000      2.000000      2.000000      1.000000\n",
      "found 4135 data\n",
      "                 ID      label_0      label_1      label_2      label_3\n",
      "count   4135.000000  4135.000000  4135.000000  4135.000000  4135.000000\n",
      "mean    8232.242080     1.240871     0.073277     0.927932     0.922370\n",
      "std     4775.436567     0.716131     0.363687     0.943816     0.267621\n",
      "min        1.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%     4074.000000     1.000000     0.000000     0.000000     1.000000\n",
      "50%     8300.000000     1.000000     0.000000     1.000000     1.000000\n",
      "75%    12337.000000     1.000000     0.000000     2.000000     1.000000\n",
      "max    16540.000000     3.000000     2.000000     2.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b8c93b10db4ec09b72da1479943c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.6213753897298561 type_f1: 0.24188894220479717 polarity_f1: 0.3357429235871259 tense_f1: 0.40511122222127754 certainty_f1: 0.4928047794430177 whole_f1: 0.37201849353363536\n",
      "train_Loss: 0.6213753897298561 weighted_f1: 0.3688869668640546 whole_f1: 0.37201849353363536 type_f1: 0.24188894220479717 polarity_f: 0.3357429235871259 tense_f1: 0.40511122222127754 certainty_f1: 0.4928047794430177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d629189f8a426b9a9fd89fb730b092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.43853401427113303 type_f1: 0.22521265284423178 polarity_f1: 0.3263374485596708 tense_f1: 0.5223537146614069 certainty_f1: 0.4798087809787394 whole_f1: 0.5055099694836876\n",
      "val_Loss: 0.43853401427113303 val_weighted_f1: 0.38842814926101227 val_whole_f1: 0.5055099694836876 val_type_f1: 0.22521265284423178 val_polarity_f: 0.3263374485596708 val_tense_f1: 0.5223537146614069 val_certainty_f1: 0.4798087809787394\n",
      "val_f1 improved!! saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902d42ceba164d3fa120270509700d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3881092875902833 type_f1: 0.24717537082549712 polarity_f1: 0.3253830706170394 tense_f1: 0.5950918729163336 certainty_f1: 0.48231958637187783 whole_f1: 0.5691365828816624\n",
      "train_Loss: 0.3881092875902833 weighted_f1: 0.412492475182687 whole_f1: 0.5691365828816624 type_f1: 0.24717537082549712 polarity_f: 0.3253830706170394 tense_f1: 0.5950918729163336 certainty_f1: 0.48231958637187783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e700d5e360c84e32af2c7378dc3baff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3111402092778553 type_f1: 0.2869479960736466 polarity_f1: 0.3263374485596708 tense_f1: 0.7035243676184443 certainty_f1: 0.5266291775248022 whole_f1: 0.6135537758720294\n",
      "val_Loss: 0.3111402092778553 val_weighted_f1: 0.46085974744414093 val_whole_f1: 0.6135537758720294 val_type_f1: 0.2869479960736466 val_polarity_f: 0.3263374485596708 val_tense_f1: 0.7035243676184443 val_certainty_f1: 0.5266291775248022\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a76abfff86540f89a877f0d93ba03ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3075000625136743 type_f1: 0.3348892016024085 polarity_f1: 0.32536898719377155 tense_f1: 0.7679376841490182 certainty_f1: 0.6211961780926165 whole_f1: 0.6299996511801588\n",
      "train_Loss: 0.3075000625136743 weighted_f1: 0.5123480127594537 whole_f1: 0.6299996511801588 type_f1: 0.3348892016024085 polarity_f: 0.32536898719377155 tense_f1: 0.7679376841490182 certainty_f1: 0.6211961780926165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6aeb566c5845f6bf0f9b4a37e85525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.266131782156957 type_f1: 0.42115461650649016 polarity_f1: 0.3263374485596708 tense_f1: 0.8380920902287173 certainty_f1: 0.6954436624978215 whole_f1: 0.6633063170708651\n",
      "val_Loss: 0.266131782156957 val_weighted_f1: 0.5702569544481749 val_whole_f1: 0.6633063170708651 val_type_f1: 0.42115461650649016 val_polarity_f: 0.3263374485596708 val_tense_f1: 0.8380920902287173 val_certainty_f1: 0.6954436624978215\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b1281cf75040a9983d6021d9aee688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2691950707133501 type_f1: 0.4589189236598372 polarity_f1: 0.35053187785913614 tense_f1: 0.8212804083194368 certainty_f1: 0.7001977599723377 whole_f1: 0.6615642063602578\n",
      "train_Loss: 0.2691950707133501 weighted_f1: 0.582732242452687 whole_f1: 0.6615642063602578 type_f1: 0.4589189236598372 polarity_f: 0.35053187785913614 tense_f1: 0.8212804083194368 certainty_f1: 0.7001977599723377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb93f1265a114d238f7eac3decfb98fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.23911950166597332 type_f1: 0.619048825945033 polarity_f1: 0.4216646195059995 tense_f1: 0.8432286596010723 certainty_f1: 0.7301785586864511 whole_f1: 0.6940398993428548\n",
      "val_Loss: 0.23911950166597332 val_weighted_f1: 0.653530165934639 val_whole_f1: 0.6940398993428548 val_type_f1: 0.619048825945033 val_polarity_f: 0.4216646195059995 val_tense_f1: 0.8432286596010723 val_certainty_f1: 0.7301785586864511\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba128ede357f4e2ba20978240f28b258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.24495105268098877 type_f1: 0.6399261209500523 polarity_f1: 0.4847514482808601 tense_f1: 0.8324417205728234 certainty_f1: 0.7169028794894452 whole_f1: 0.6965499240683952\n",
      "train_Loss: 0.24495105268098877 weighted_f1: 0.6685055423232953 whole_f1: 0.6965499240683952 type_f1: 0.6399261209500523 polarity_f: 0.4847514482808601 tense_f1: 0.8324417205728234 certainty_f1: 0.7169028794894452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21d8321bda0426683057faab52c606e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22465609609720352 type_f1: 0.6770126428752687 polarity_f1: 0.5908934525753072 tense_f1: 0.8482313983246142 certainty_f1: 0.7204339195085399 whole_f1: 0.7151164294121329\n",
      "val_Loss: 0.22465609609720352 val_weighted_f1: 0.7091428533209325 val_whole_f1: 0.7151164294121329 val_type_f1: 0.6770126428752687 val_polarity_f: 0.5908934525753072 val_tense_f1: 0.8482313983246142 val_certainty_f1: 0.7204339195085399\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25482bc66b649f9b117d60bd04c2a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22713062889044003 type_f1: 0.7099001417895026 polarity_f1: 0.5822550668749809 tense_f1: 0.8370101165159065 certainty_f1: 0.7266347701289333 whole_f1: 0.7179496134083194\n",
      "train_Loss: 0.22713062889044003 weighted_f1: 0.7139500238273307 whole_f1: 0.7179496134083194 type_f1: 0.7099001417895026 polarity_f: 0.5822550668749809 tense_f1: 0.8370101165159065 certainty_f1: 0.7266347701289333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a314458e084f918ff52aaad3369937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21385279856036624 type_f1: 0.7293353468389644 polarity_f1: 0.6693636488358856 tense_f1: 0.8536914079616652 certainty_f1: 0.7435391373079964 whole_f1: 0.7292375519109792\n",
      "val_Loss: 0.21385279856036624 val_weighted_f1: 0.7489823852361279 val_whole_f1: 0.7292375519109792 val_type_f1: 0.7293353468389644 val_polarity_f: 0.6693636488358856 val_tense_f1: 0.8536914079616652 val_certainty_f1: 0.7435391373079964\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce6fdc052b94721a255982f896a9269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21391259177296504 type_f1: 0.7547591679418812 polarity_f1: 0.6041046405800676 tense_f1: 0.849177651017255 certainty_f1: 0.7324811040767791 whole_f1: 0.7331149395234917\n",
      "train_Loss: 0.21391259177296504 weighted_f1: 0.7351306409039957 whole_f1: 0.7331149395234917 type_f1: 0.7547591679418812 polarity_f: 0.6041046405800676 tense_f1: 0.849177651017255 certainty_f1: 0.7324811040767791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c1b331549b454f837532d66a781ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21046818910299506 type_f1: 0.7273474881364946 polarity_f1: 0.6965547892387974 tense_f1: 0.8498124790689047 certainty_f1: 0.7454781831781141 whole_f1: 0.7305606047139996\n",
      "val_Loss: 0.21046818910299506 val_weighted_f1: 0.7547982349055777 val_whole_f1: 0.7305606047139996 val_type_f1: 0.7273474881364946 val_polarity_f: 0.6965547892387974 val_tense_f1: 0.8498124790689047 val_certainty_f1: 0.7454781831781141\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d25a8cc01049ee8160ee34fe4ad8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2049967546136691 type_f1: 0.7581904481134456 polarity_f1: 0.6507207588901824 tense_f1: 0.8564958432845226 certainty_f1: 0.7404678145893873 whole_f1: 0.7415366065550981\n",
      "train_Loss: 0.2049967546136691 weighted_f1: 0.7514687162193845 whole_f1: 0.7415366065550981 type_f1: 0.7581904481134456 polarity_f: 0.6507207588901824 tense_f1: 0.8564958432845226 certainty_f1: 0.7404678145893873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9b3bf1d217449fa442723d14fe80a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20500177936729813 type_f1: 0.7449348405141261 polarity_f1: 0.6998133336016193 tense_f1: 0.8573477966395734 certainty_f1: 0.7312964751376134 whole_f1: 0.7310459391860777\n",
      "val_Loss: 0.20500177936729813 val_weighted_f1: 0.7583481114732331 val_whole_f1: 0.7310459391860777 val_type_f1: 0.7449348405141261 val_polarity_f: 0.6998133336016193 val_tense_f1: 0.8573477966395734 val_certainty_f1: 0.7312964751376134\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91b8d7dae742ca92bb8a840631f08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20089773943534536 type_f1: 0.7623679224972453 polarity_f1: 0.6648483867421017 tense_f1: 0.8578894559155144 certainty_f1: 0.7544607766781666 whole_f1: 0.7436649533723693\n",
      "train_Loss: 0.20089773943534536 weighted_f1: 0.759891635458257 whole_f1: 0.7436649533723693 type_f1: 0.7623679224972453 polarity_f: 0.6648483867421017 tense_f1: 0.8578894559155144 certainty_f1: 0.7544607766781666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3cd794345d4d4cbeafe5949335370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20548399760688518 type_f1: 0.747328821309888 polarity_f1: 0.7102084150220876 tense_f1: 0.8544062090168385 certainty_f1: 0.7454781831781141 whole_f1: 0.7326737087624939\n",
      "val_Loss: 0.20548399760688518 val_weighted_f1: 0.764355407131732 val_whole_f1: 0.7326737087624939 val_type_f1: 0.747328821309888 val_polarity_f: 0.7102084150220876 val_tense_f1: 0.8544062090168385 val_certainty_f1: 0.7454781831781141\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf0c316a8f343aa9988f785d5c22e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19126522320039077 type_f1: 0.7864124654145328 polarity_f1: 0.6898278589921502 tense_f1: 0.8643370158869498 certainty_f1: 0.7601483722428601 whole_f1: 0.7534456799682259\n",
      "train_Loss: 0.19126522320039077 weighted_f1: 0.7751814281341233 whole_f1: 0.7534456799682259 type_f1: 0.7864124654145328 polarity_f: 0.6898278589921502 tense_f1: 0.8643370158869498 certainty_f1: 0.7601483722428601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8575073f6d2048d58ec2054f76ee0047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20310056352269404 type_f1: 0.7694039909287165 polarity_f1: 0.7320878215150567 tense_f1: 0.8530767582394648 certainty_f1: 0.7401075128214205 whole_f1: 0.7397224424971993\n",
      "val_Loss: 0.20310056352269404 val_weighted_f1: 0.7736690208761646 val_whole_f1: 0.7397224424971993 val_type_f1: 0.7694039909287165 val_polarity_f: 0.7320878215150567 val_tense_f1: 0.8530767582394648 val_certainty_f1: 0.7401075128214205\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82557f6a45ee4f8dab7de9bea85faabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1869028293589671 type_f1: 0.7884722467461329 polarity_f1: 0.6781121962562257 tense_f1: 0.8690926888916719 certainty_f1: 0.7553962493835417 whole_f1: 0.7583709065484905\n",
      "train_Loss: 0.1869028293589671 weighted_f1: 0.772768345319393 whole_f1: 0.7583709065484905 type_f1: 0.7884722467461329 polarity_f: 0.6781121962562257 tense_f1: 0.8690926888916719 certainty_f1: 0.7553962493835417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8926ce0eb24509952265bedd045e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20237206060984403 type_f1: 0.7629867304802533 polarity_f1: 0.7188707188371167 tense_f1: 0.8522776915947134 certainty_f1: 0.7526055021439612 whole_f1: 0.7372307011470219\n",
      "val_Loss: 0.20237206060984403 val_weighted_f1: 0.7716851607640112 val_whole_f1: 0.7372307011470219 val_type_f1: 0.7629867304802533 val_polarity_f: 0.7188707188371167 val_tense_f1: 0.8522776915947134 val_certainty_f1: 0.7526055021439612\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445b72c182fb4a4a83924a38d0990167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.177291533091021 type_f1: 0.7953286107691212 polarity_f1: 0.6861841265544264 tense_f1: 0.8800304919385121 certainty_f1: 0.7632385523628439 whole_f1: 0.7664732151436867\n",
      "train_Loss: 0.177291533091021 weighted_f1: 0.7811954454062259 whole_f1: 0.7664732151436867 type_f1: 0.7953286107691212 polarity_f: 0.6861841265544264 tense_f1: 0.8800304919385121 certainty_f1: 0.7632385523628439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dd8c804345487f8d69b2f54e37f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20480127709736415 type_f1: 0.7608434108188755 polarity_f1: 0.7170949203749793 tense_f1: 0.8522806302896374 certainty_f1: 0.7480424449019356 whole_f1: 0.7338632413751949\n",
      "val_Loss: 0.20480127709736415 val_weighted_f1: 0.769565351596357 val_whole_f1: 0.7338632413751949 val_type_f1: 0.7608434108188755 val_polarity_f: 0.7170949203749793 val_tense_f1: 0.8522806302896374 val_certainty_f1: 0.7480424449019356\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708c7a842fe49c49470520225a6ec5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17467021412682615 type_f1: 0.7870780686655339 polarity_f1: 0.7123929978027751 tense_f1: 0.8775479536222157 certainty_f1: 0.7734538945050997 whole_f1: 0.7704348233466847\n",
      "train_Loss: 0.17467021412682615 weighted_f1: 0.7876182286489061 whole_f1: 0.7704348233466847 type_f1: 0.7870780686655339 polarity_f: 0.7123929978027751 tense_f1: 0.8775479536222157 certainty_f1: 0.7734538945050997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13153ea7f2a64235b6bf99cdef90c055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20182328649414782 type_f1: 0.7575518312643647 polarity_f1: 0.7198845375220263 tense_f1: 0.8514245048417117 certainty_f1: 0.7501404247044967 whole_f1: 0.7409731556624786\n",
      "val_Loss: 0.20182328649414782 val_weighted_f1: 0.7697503245831498 val_whole_f1: 0.7409731556624786 val_type_f1: 0.7575518312643647 val_polarity_f: 0.7198845375220263 val_tense_f1: 0.8514245048417117 val_certainty_f1: 0.7501404247044967\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30d56da6edb49d5add75444deaa39a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.16798725262988137 type_f1: 0.812938788553351 polarity_f1: 0.7023563063333257 tense_f1: 0.8814653818444561 certainty_f1: 0.7701813815085183 whole_f1: 0.7771834289024074\n",
      "train_Loss: 0.16798725262988137 weighted_f1: 0.7917354645599128 whole_f1: 0.7771834289024074 type_f1: 0.812938788553351 polarity_f: 0.7023563063333257 tense_f1: 0.8814653818444561 certainty_f1: 0.7701813815085183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ff718434ca448f989e2c114ad33c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20472601722597067 type_f1: 0.7587624028536055 polarity_f1: 0.7313583046456656 tense_f1: 0.8602525695316512 certainty_f1: 0.752861286291143 whole_f1: 0.7427592417737686\n",
      "val_Loss: 0.20472601722597067 val_weighted_f1: 0.7758086408305164 val_whole_f1: 0.7427592417737686 val_type_f1: 0.7587624028536055 val_polarity_f: 0.7313583046456656 val_tense_f1: 0.8602525695316512 val_certainty_f1: 0.752861286291143\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755551df08834736b8e4723ed5b68028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.16153219868564883 type_f1: 0.8207194468144969 polarity_f1: 0.7248961167548759 tense_f1: 0.8868022276329034 certainty_f1: 0.7796475341409591 whole_f1: 0.7848150834185523\n",
      "train_Loss: 0.16153219868564883 weighted_f1: 0.8030163313358089 whole_f1: 0.7848150834185523 type_f1: 0.8207194468144969 polarity_f: 0.7248961167548759 tense_f1: 0.8868022276329034 certainty_f1: 0.7796475341409591\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b86e3bf304b492cb39e69dffea6baef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20580959528360285 type_f1: 0.7586428031970768 polarity_f1: 0.7378979752280997 tense_f1: 0.8530939828500087 certainty_f1: 0.7471650917176209 whole_f1: 0.7408549586950911\n",
      "val_Loss: 0.20580959528360285 val_weighted_f1: 0.7741999632482015 val_whole_f1: 0.7408549586950911 val_type_f1: 0.7586428031970768 val_polarity_f: 0.7378979752280997 val_tense_f1: 0.8530939828500087 val_certainty_f1: 0.7471650917176209\n",
      "val_accuracy is not improved...\n",
      "found 7090 data\n",
      "                ID  label_0  label_1  label_2  label_3\n",
      "count  7090.000000   7090.0   7090.0   7090.0   7090.0\n",
      "mean    493.724260      1.0      0.0      2.0      1.0\n",
      "std     291.359857      0.0      0.0      0.0      0.0\n",
      "min       0.000000      1.0      0.0      2.0      1.0\n",
      "25%     240.000000      1.0      0.0      2.0      1.0\n",
      "50%     493.000000      1.0      0.0      2.0      1.0\n",
      "75%     746.000000      1.0      0.0      2.0      1.0\n",
      "max     999.000000      1.0      0.0      2.0      1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9901391f78834012bbba811109f34a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_md_txt(klue_bert_base)md_img(NaN)b(128)L(5e-06)ml(128)d(0)a(0.1)an(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12406 data\n",
      "                 ID       label_0       label_1       label_2       label_3\n",
      "count  12406.000000  12406.000000  12406.000000  12406.000000  12406.000000\n",
      "mean    8246.017330      1.240851      0.078188      0.924069      0.919877\n",
      "std     4771.830381      0.715822      0.373676      0.946981      0.271494\n",
      "min        1.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%     4095.250000      1.000000      0.000000      0.000000      1.000000\n",
      "50%     8266.500000      1.000000      0.000000      1.000000      1.000000\n",
      "75%    12322.500000      1.000000      0.000000      2.000000      1.000000\n",
      "max    16540.000000      3.000000      2.000000      2.000000      1.000000\n",
      "found 4135 data\n",
      "                 ID      label_0      label_1      label_2      label_3\n",
      "count   4135.000000  4135.000000  4135.000000  4135.000000  4135.000000\n",
      "mean    8341.953809     1.240871     0.082950     0.945828     0.914148\n",
      "std     4784.831475     0.716131     0.383043     0.944757     0.280180\n",
      "min        0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%     4254.500000     1.000000     0.000000     0.000000     1.000000\n",
      "50%     8283.000000     1.000000     0.000000     1.000000     1.000000\n",
      "75%    12583.000000     1.000000     0.000000     2.000000     1.000000\n",
      "max    16537.000000     3.000000     2.000000     2.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7a4e5ace314330af419b065154725a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.6624872865800644 type_f1: 0.22894635188116802 polarity_f1: 0.3308733684689253 tense_f1: 0.3839300469599079 certainty_f1: 0.4894679891606973 whole_f1: 0.35245911731596513\n",
      "train_Loss: 0.6624872865800644 weighted_f1: 0.3583044391176746 whole_f1: 0.35245911731596513 type_f1: 0.22894635188116802 polarity_f: 0.3308733684689253 tense_f1: 0.3839300469599079 certainty_f1: 0.4894679891606973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6288593116345e7998c7cbd20aa81e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.48143552484794855 type_f1: 0.22521265284423178 polarity_f1: 0.32519922374994836 tense_f1: 0.4615849502026383 certainty_f1: 0.47757422615287426 whole_f1: 0.44471731564488537\n",
      "val_Loss: 0.48143552484794855 val_weighted_f1: 0.3723927632374232 val_whole_f1: 0.44471731564488537 val_type_f1: 0.22521265284423178 val_polarity_f: 0.32519922374994836 val_tense_f1: 0.4615849502026383 val_certainty_f1: 0.47757422615287426\n",
      "val_f1 improved!! saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bff8916d2b42e6b030c96f85b2d88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.4199452019883186 type_f1: 0.24386538253910806 polarity_f1: 0.3257628841899894 tense_f1: 0.5529966247781357 certainty_f1: 0.48013569224833497 whole_f1: 0.5347699391852596\n",
      "train_Loss: 0.4199452019883186 weighted_f1: 0.4006901459388921 whole_f1: 0.5347699391852596 type_f1: 0.24386538253910806 polarity_f: 0.3257628841899894 tense_f1: 0.5529966247781357 certainty_f1: 0.48013569224833497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaebf76da4b4c3699363b94a543d7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.32919255373988215 type_f1: 0.31593507695173206 polarity_f1: 0.32519922374994836 tense_f1: 0.6744962997020063 certainty_f1: 0.47757422615287426 whole_f1: 0.6036151906620708\n",
      "val_Loss: 0.32919255373988215 val_weighted_f1: 0.4483012066391403 val_whole_f1: 0.6036151906620708 val_type_f1: 0.31593507695173206 val_polarity_f: 0.32519922374994836 val_tense_f1: 0.6744962997020063 val_certainty_f1: 0.47757422615287426\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a066e19a8d8479a974d755e8572f253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.31480686547928005 type_f1: 0.32485053159552263 polarity_f1: 0.3257628841899894 tense_f1: 0.7639694704600611 certainty_f1: 0.5483633716022286 whole_f1: 0.6265850792750575\n",
      "train_Loss: 0.31480686547928005 weighted_f1: 0.49073656446195046 whole_f1: 0.6265850792750575 type_f1: 0.32485053159552263 polarity_f: 0.3257628841899894 tense_f1: 0.7639694704600611 certainty_f1: 0.5483633716022286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d17ba876d4c4ca193285af5253d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2764399295994852 type_f1: 0.37453212034546574 polarity_f1: 0.3297746584593534 tense_f1: 0.8383809968160923 certainty_f1: 0.6357044748680084 whole_f1: 0.6474184635298085\n",
      "val_Loss: 0.2764399295994852 val_weighted_f1: 0.54459806262223 val_whole_f1: 0.6474184635298085 val_type_f1: 0.37453212034546574 val_polarity_f: 0.3297746584593534 val_tense_f1: 0.8383809968160923 val_certainty_f1: 0.6357044748680084\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281dfa1ddb2f4bcc908e589ab742651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2726219591691996 type_f1: 0.4426228051417192 polarity_f1: 0.3621349522488324 tense_f1: 0.8224676750935239 certainty_f1: 0.6643921406744575 whole_f1: 0.6585526593192255\n",
      "train_Loss: 0.2726219591691996 weighted_f1: 0.5729043932896332 whole_f1: 0.6585526593192255 type_f1: 0.4426228051417192 polarity_f: 0.3621349522488324 tense_f1: 0.8224676750935239 certainty_f1: 0.6643921406744575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaedf9f68a954425b0accb44c70df92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.24866280667078394 type_f1: 0.5441068513558168 polarity_f1: 0.4323023415968545 tense_f1: 0.8457874099411468 certainty_f1: 0.7003392299535095 whole_f1: 0.6762719758278204\n",
      "val_Loss: 0.24866280667078394 val_weighted_f1: 0.6306339582118319 val_whole_f1: 0.6762719758278204 val_type_f1: 0.5441068513558168 val_polarity_f: 0.4323023415968545 val_tense_f1: 0.8457874099411468 val_certainty_f1: 0.7003392299535095\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b5626678754cce8dc162d23f133fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.247124648756718 type_f1: 0.5861932800488765 polarity_f1: 0.5029253571521987 tense_f1: 0.8298043392334858 certainty_f1: 0.7057752326607297 whole_f1: 0.6923492008068515\n",
      "train_Loss: 0.247124648756718 weighted_f1: 0.6561745522738227 whole_f1: 0.6923492008068515 type_f1: 0.5861932800488765 polarity_f: 0.5029253571521987 tense_f1: 0.8298043392334858 certainty_f1: 0.7057752326607297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177e77fc664f433fa42604182875805e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22928553080003813 type_f1: 0.7168374776862789 polarity_f1: 0.570108172255463 tense_f1: 0.8449450385288982 certainty_f1: 0.7165839104371391 whole_f1: 0.7166959058270185\n",
      "val_Loss: 0.22928553080003813 val_weighted_f1: 0.7121186497269448 val_whole_f1: 0.7166959058270185 val_type_f1: 0.7168374776862789 val_polarity_f: 0.570108172255463 val_tense_f1: 0.8449450385288982 val_certainty_f1: 0.7165839104371391\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23caca5ce18443a69d2d6e82a1e8daca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22782005273813435 type_f1: 0.6866865191877652 polarity_f1: 0.5673564466365818 tense_f1: 0.8473525880680159 certainty_f1: 0.7123066743905848 whole_f1: 0.7184213749297049\n",
      "train_Loss: 0.22782005273813435 weighted_f1: 0.7034255570707371 whole_f1: 0.7184213749297049 type_f1: 0.6866865191877652 polarity_f: 0.5673564466365818 tense_f1: 0.8473525880680159 certainty_f1: 0.7123066743905848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234c7e259484403f9a0bd1af1a5717eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22447626073962976 type_f1: 0.7280935047883741 polarity_f1: 0.5783160320312529 tense_f1: 0.8479620188504772 certainty_f1: 0.7088567722808968 whole_f1: 0.7121851586119747\n",
      "val_Loss: 0.22447626073962976 val_weighted_f1: 0.7158070819877502 val_whole_f1: 0.7121851586119747 val_type_f1: 0.7280935047883741 val_polarity_f: 0.5783160320312529 val_tense_f1: 0.8479620188504772 val_certainty_f1: 0.7088567722808968\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258eee4bba4440a4b9aabbc3b0f41fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21635829662721273 type_f1: 0.7355495773652696 polarity_f1: 0.5945735174108543 tense_f1: 0.8464855011553997 certainty_f1: 0.7185764477302286 whole_f1: 0.7250308723352052\n",
      "train_Loss: 0.21635829662721273 weighted_f1: 0.723796260915438 whole_f1: 0.7250308723352052 type_f1: 0.7355495773652696 polarity_f: 0.5945735174108543 tense_f1: 0.8464855011553997 certainty_f1: 0.7185764477302286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4743ebe669724b9b9debd4265893d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2181261743787122 type_f1: 0.7564118708315329 polarity_f1: 0.6125918240295777 tense_f1: 0.8494014677152412 certainty_f1: 0.736643374458638 whole_f1: 0.7201392415950308\n",
      "val_Loss: 0.2181261743787122 val_weighted_f1: 0.7387621342587474 val_whole_f1: 0.7201392415950308 val_type_f1: 0.7564118708315329 val_polarity_f: 0.6125918240295777 val_tense_f1: 0.8494014677152412 val_certainty_f1: 0.736643374458638\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af178e08a04149ddbdf95bc1c4499d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20715830696939788 type_f1: 0.7349988056816836 polarity_f1: 0.6050972753860171 tense_f1: 0.8533233504156265 certainty_f1: 0.7350123116741054 whole_f1: 0.7367822940428265\n",
      "train_Loss: 0.20715830696939788 weighted_f1: 0.7321079357893582 whole_f1: 0.7367822940428265 type_f1: 0.7349988056816836 polarity_f: 0.6050972753860171 tense_f1: 0.8533233504156265 certainty_f1: 0.7350123116741054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e1f76381254db18982d71d83458690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2124005245866914 type_f1: 0.7537748399601014 polarity_f1: 0.6677182227630517 tense_f1: 0.8503035072451225 certainty_f1: 0.7305095396181929 whole_f1: 0.7262144559012228\n",
      "val_Loss: 0.2124005245866914 val_weighted_f1: 0.7505765273966172 val_whole_f1: 0.7262144559012228 val_type_f1: 0.7537748399601014 val_polarity_f: 0.6677182227630517 val_tense_f1: 0.8503035072451225 val_certainty_f1: 0.7305095396181929\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da69b65340b485a955c7a7c0869b796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20063229294125348 type_f1: 0.7548084142182979 polarity_f1: 0.6338305788441223 tense_f1: 0.8580343783633081 certainty_f1: 0.747436280193942 whole_f1: 0.7420997259196541\n",
      "train_Loss: 0.20063229294125348 weighted_f1: 0.7485274129049175 whole_f1: 0.7420997259196541 type_f1: 0.7548084142182979 polarity_f: 0.6338305788441223 tense_f1: 0.8580343783633081 certainty_f1: 0.747436280193942\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7af2eef4e143619dc46e299081c1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21083515621406423 type_f1: 0.7719532291676418 polarity_f1: 0.6622725398291253 tense_f1: 0.8461592416285407 certainty_f1: 0.71784528474037 whole_f1: 0.7234179397636249\n",
      "val_Loss: 0.21083515621406423 val_weighted_f1: 0.7495575738414194 val_whole_f1: 0.7234179397636249 val_type_f1: 0.7719532291676418 val_polarity_f: 0.6622725398291253 val_tense_f1: 0.8461592416285407 val_certainty_f1: 0.71784528474037\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4570bd710c4147f8ac2d16233f941eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19201013299557118 type_f1: 0.7673474152832405 polarity_f1: 0.6754323017843676 tense_f1: 0.870679985413272 certainty_f1: 0.753478050146476 whole_f1: 0.7551610717592591\n",
      "train_Loss: 0.19201013299557118 weighted_f1: 0.7667344381568391 whole_f1: 0.7551610717592591 type_f1: 0.7673474152832405 polarity_f: 0.6754323017843676 tense_f1: 0.870679985413272 certainty_f1: 0.753478050146476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c36e758901454cb9ddeecbf8f46a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21068299112181565 type_f1: 0.7711018303808765 polarity_f1: 0.6451639177402206 tense_f1: 0.851222207997307 certainty_f1: 0.7384349161289876 whole_f1: 0.7270528777630737\n",
      "val_Loss: 0.21068299112181565 val_weighted_f1: 0.7514807180618479 val_whole_f1: 0.7270528777630737 val_type_f1: 0.7711018303808765 val_polarity_f: 0.6451639177402206 val_tense_f1: 0.851222207997307 val_certainty_f1: 0.7384349161289876\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ae6e375e148c5bc5b39d27a90af21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.18637216212709892 type_f1: 0.7791401582752667 polarity_f1: 0.6535854073149678 tense_f1: 0.8745553388781414 certainty_f1: 0.7462509841491716 whole_f1: 0.7606575745454259\n",
      "train_Loss: 0.18637216212709892 weighted_f1: 0.763382972154387 whole_f1: 0.7606575745454259 type_f1: 0.7791401582752667 polarity_f: 0.6535854073149678 tense_f1: 0.8745553388781414 certainty_f1: 0.7462509841491716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff80c045fb2b42a7ad02e671aaf53bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21218325746354919 type_f1: 0.7826912990347121 polarity_f1: 0.6517304991870244 tense_f1: 0.8502253751388835 certainty_f1: 0.7563263114730532 whole_f1: 0.7283521649074689\n",
      "val_Loss: 0.21218325746354919 val_weighted_f1: 0.7602433712084183 val_whole_f1: 0.7283521649074689 val_type_f1: 0.7826912990347121 val_polarity_f: 0.6517304991870244 val_tense_f1: 0.8502253751388835 val_certainty_f1: 0.7563263114730532\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc71983be504f5ea503b7ea3382b9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1798037079030922 type_f1: 0.7866520678185154 polarity_f1: 0.6824855601381626 tense_f1: 0.8777652481494398 certainty_f1: 0.7598892757997117 whole_f1: 0.7710833684618184\n",
      "train_Loss: 0.1798037079030922 weighted_f1: 0.7766980379764574 whole_f1: 0.7710833684618184 type_f1: 0.7866520678185154 polarity_f: 0.6824855601381626 tense_f1: 0.8777652481494398 certainty_f1: 0.7598892757997117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54c6405a3bf44ea946cbec19ae377df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20949776239395862 type_f1: 0.7870596961152521 polarity_f1: 0.6761599959797943 tense_f1: 0.8479479320276292 certainty_f1: 0.7446357622120805 whole_f1: 0.7307329208812527\n",
      "val_Loss: 0.20949776239395862 val_weighted_f1: 0.7639508465836891 val_whole_f1: 0.7307329208812527 val_type_f1: 0.7870596961152521 val_polarity_f: 0.6761599959797943 val_tense_f1: 0.8479479320276292 val_certainty_f1: 0.7446357622120805\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a1e24233374065bfaae982e080081e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17269011085674915 type_f1: 0.7933940525199177 polarity_f1: 0.6771757807524746 tense_f1: 0.8820779999073157 certainty_f1: 0.7632262952759852 whole_f1: 0.7736969070035421\n",
      "train_Loss: 0.17269011085674915 weighted_f1: 0.7789685321139233 whole_f1: 0.7736969070035421 type_f1: 0.7933940525199177 polarity_f: 0.6771757807524746 tense_f1: 0.8820779999073157 certainty_f1: 0.7632262952759852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d61e42501904999ae1a84b3e29235a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21119950249376723 type_f1: 0.7799336756462627 polarity_f1: 0.6494547597995873 tense_f1: 0.8536106174869588 certainty_f1: 0.7269441178123116 whole_f1: 0.7256231857791885\n",
      "val_Loss: 0.21119950249376723 val_weighted_f1: 0.75248579268628 val_whole_f1: 0.7256231857791885 val_type_f1: 0.7799336756462627 val_polarity_f: 0.6494547597995873 val_tense_f1: 0.8536106174869588 val_certainty_f1: 0.7269441178123116\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9566a27a8de2488686f3e674dd43f3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.16743111413428038 type_f1: 0.7910868825347274 polarity_f1: 0.6968882735024469 tense_f1: 0.8949187862253009 certainty_f1: 0.781185011621879 whole_f1: 0.7833326989408701\n",
      "train_Loss: 0.16743111413428038 weighted_f1: 0.7910197384710886 whole_f1: 0.7833326989408701 type_f1: 0.7910868825347274 polarity_f: 0.6968882735024469 tense_f1: 0.8949187862253009 certainty_f1: 0.781185011621879\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba651657027d4723a14e50bc8206f98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21173714056629078 type_f1: 0.7799185549777635 polarity_f1: 0.6848687144450034 tense_f1: 0.852997940395212 certainty_f1: 0.7466877647329457 whole_f1: 0.7330459989387804\n",
      "val_Loss: 0.21173714056629078 val_weighted_f1: 0.7661182436377312 val_whole_f1: 0.7330459989387804 val_type_f1: 0.7799185549777635 val_polarity_f: 0.6848687144450034 val_tense_f1: 0.852997940395212 val_certainty_f1: 0.7466877647329457\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93325873a3c54896b862c357018aa8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.16165765824239908 type_f1: 0.8150941956976159 polarity_f1: 0.6962915330350748 tense_f1: 0.8974512281160355 certainty_f1: 0.7729646465983278 whole_f1: 0.7919823645549594\n",
      "train_Loss: 0.16165765824239908 weighted_f1: 0.7954504008617636 whole_f1: 0.7919823645549594 type_f1: 0.8150941956976159 polarity_f: 0.6962915330350748 tense_f1: 0.8974512281160355 certainty_f1: 0.7729646465983278\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21da98bfffe4a419ed82f27a553fd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21423783583573303 type_f1: 0.7757578394168051 polarity_f1: 0.6761145036220779 tense_f1: 0.8544149203742006 certainty_f1: 0.7326952129458917 whole_f1: 0.7298838190706838\n",
      "val_Loss: 0.21423783583573303 val_weighted_f1: 0.7597456190897438 val_whole_f1: 0.7298838190706838 val_type_f1: 0.7757578394168051 val_polarity_f: 0.6761145036220779 val_tense_f1: 0.8544149203742006 val_certainty_f1: 0.7326952129458917\n",
      "val_accuracy is not improved...\n",
      "found 7090 data\n",
      "                ID  label_0  label_1  label_2  label_3\n",
      "count  7090.000000   7090.0   7090.0   7090.0   7090.0\n",
      "mean    493.724260      1.0      0.0      2.0      1.0\n",
      "std     291.359857      0.0      0.0      0.0      0.0\n",
      "min       0.000000      1.0      0.0      2.0      1.0\n",
      "25%     240.000000      1.0      0.0      2.0      1.0\n",
      "50%     493.000000      1.0      0.0      2.0      1.0\n",
      "75%     746.000000      1.0      0.0      2.0      1.0\n",
      "max     999.000000      1.0      0.0      2.0      1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5446e4cd7d934123a34242f636981bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\torch\\cuda\\memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_md_txt(klue_bert_base)md_img(NaN)b(128)L(5e-06)ml(128)d(0)a(0)an(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12405 data\n",
      "                 ID       label_0       label_1       label_2       label_3\n",
      "count  12405.000000  12405.000000  12405.000000  12405.000000  12405.000000\n",
      "mean    8290.676098      1.240871      0.080693      0.934220      0.918339\n",
      "std     4776.637412      0.715848      0.379266      0.945309      0.273858\n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
      "25%     4175.000000      1.000000      0.000000      0.000000      1.000000\n",
      "50%     8291.000000      1.000000      0.000000      1.000000      1.000000\n",
      "75%    12448.000000      1.000000      0.000000      2.000000      1.000000\n",
      "max    16540.000000      3.000000      2.000000      2.000000      1.000000\n",
      "found 4136 data\n",
      "                 ID      label_0      label_1      label_2      label_3\n",
      "count   4136.000000  4136.000000  4136.000000  4136.000000  4136.000000\n",
      "mean    8207.986702     1.240812     0.075435     0.915377     0.918762\n",
      "std     4770.604558     0.716054     0.366184     0.949815     0.273233\n",
      "min        4.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%     4012.750000     1.000000     0.000000     0.000000     1.000000\n",
      "50%     8195.000000     1.000000     0.000000     1.000000     1.000000\n",
      "75%    12296.500000     1.000000     0.000000     2.000000     1.000000\n",
      "max    16539.000000     3.000000     2.000000     2.000000     1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab91f42a5ef4b6d8dbcb477632acee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.6217951044422251 type_f1: 0.2392259246338571 polarity_f1: 0.3287802714782863 tense_f1: 0.4110489455446787 certainty_f1: 0.4977178697911495 whole_f1: 0.38381782870344305\n",
      "train_Loss: 0.6217951044422251 weighted_f1: 0.3691932528619929 whole_f1: 0.38381782870344305 type_f1: 0.2392259246338571 polarity_f: 0.3287802714782863 tense_f1: 0.4110489455446787 certainty_f1: 0.4977178697911495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca03d74306304d26a6947f0f943b545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.44594179748105356 type_f1: 0.22521923996811055 polarity_f1: 0.32596070678364014 tense_f1: 0.5179724145314181 certainty_f1: 0.4788306451612903 whole_f1: 0.5085026757682227\n",
      "val_Loss: 0.44594179748105356 val_weighted_f1: 0.3869957516111147 val_whole_f1: 0.5085026757682227 val_type_f1: 0.22521923996811055 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.5179724145314181 val_certainty_f1: 0.4788306451612903\n",
      "val_f1 improved!! saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0bee03fe15449a997ef00a1cd33e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.3940437494963708 type_f1: 0.24016762196747538 polarity_f1: 0.3255091236610151 tense_f1: 0.591042290946666 certainty_f1: 0.47867198991384746 whole_f1: 0.5645055450995288\n",
      "train_Loss: 0.3940437494963708 weighted_f1: 0.408847756622251 whole_f1: 0.5645055450995288 type_f1: 0.24016762196747538 polarity_f: 0.3255091236610151 tense_f1: 0.591042290946666 certainty_f1: 0.47867198991384746\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aee89ea478745749a2b372b9226a3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.31677816038666656 type_f1: 0.25666990011324686 polarity_f1: 0.32596070678364014 tense_f1: 0.715856701826365 certainty_f1: 0.4848685083894794 whole_f1: 0.6120204366062586\n",
      "val_Loss: 0.31677816038666656 val_weighted_f1: 0.4458389542781829 val_whole_f1: 0.6120204366062586 val_type_f1: 0.25666990011324686 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.715856701826365 val_certainty_f1: 0.4848685083894794\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0510653a92984a11820af05e8692ae58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.308272372081077 type_f1: 0.3400531936181589 polarity_f1: 0.3255091236610151 tense_f1: 0.7636698904656206 certainty_f1: 0.5635752075115207 whole_f1: 0.6277287471847852\n",
      "train_Loss: 0.308272372081077 weighted_f1: 0.4982018538140788 whole_f1: 0.6277287471847852 type_f1: 0.3400531936181589 polarity_f: 0.3255091236610151 tense_f1: 0.7636698904656206 certainty_f1: 0.5635752075115207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27adb1ee2b82418aba11218d007dfabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.26060709154064005 type_f1: 0.4214656384305625 polarity_f1: 0.32596070678364014 tense_f1: 0.8280652072133536 certainty_f1: 0.6806502911064675 whole_f1: 0.6651077034934634\n",
      "val_Loss: 0.26060709154064005 val_weighted_f1: 0.564035460883506 val_whole_f1: 0.6651077034934634 val_type_f1: 0.4214656384305625 val_polarity_f: 0.32596070678364014 val_tense_f1: 0.8280652072133536 val_certainty_f1: 0.6806502911064675\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f9781e1f7542908b2cb68349071f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2670734687132107 type_f1: 0.5067703016586985 polarity_f1: 0.34394141386171156 tense_f1: 0.8287535253471128 certainty_f1: 0.6855226625001337 whole_f1: 0.6697456640696418\n",
      "train_Loss: 0.2670734687132107 weighted_f1: 0.5912469758419141 whole_f1: 0.6697456640696418 type_f1: 0.5067703016586985 polarity_f: 0.34394141386171156 tense_f1: 0.8287535253471128 certainty_f1: 0.6855226625001337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8e85563e5d4b709b18d980a81ce4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.22970805309957876 type_f1: 0.6665397010081904 polarity_f1: 0.37225676235017 tense_f1: 0.8522674713768859 certainty_f1: 0.7333541581974425 whole_f1: 0.7068682471626813\n",
      "val_Loss: 0.22970805309957876 val_weighted_f1: 0.6561045232331721 val_whole_f1: 0.7068682471626813 val_type_f1: 0.6665397010081904 val_polarity_f: 0.37225676235017 val_tense_f1: 0.8522674713768859 val_certainty_f1: 0.7333541581974425\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78f9d1f289b4cc9994860c6b4040b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.23881772208578977 type_f1: 0.6573067431215165 polarity_f1: 0.47778595992683054 tense_f1: 0.8451748068127106 certainty_f1: 0.7088272722503576 whole_f1: 0.7030018506499645\n",
      "train_Loss: 0.23881772208578977 weighted_f1: 0.6722736955278539 whole_f1: 0.7030018506499645 type_f1: 0.6573067431215165 polarity_f: 0.47778595992683054 tense_f1: 0.8451748068127106 certainty_f1: 0.7088272722503576\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019dc7e44f2241eeb1e3b43c92d92942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.21286606971657254 type_f1: 0.7641870650681374 polarity_f1: 0.5808115067315582 tense_f1: 0.850814784685376 certainty_f1: 0.7381165594782593 whole_f1: 0.7298601051838933\n",
      "val_Loss: 0.21286606971657254 val_weighted_f1: 0.7334824789908327 val_whole_f1: 0.7298601051838933 val_type_f1: 0.7641870650681374 val_polarity_f: 0.5808115067315582 val_tense_f1: 0.850814784685376 val_certainty_f1: 0.7381165594782593\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6fa075566c440f84622d619ca19e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2194070057203961 type_f1: 0.7107576411121799 polarity_f1: 0.5831009782589764 tense_f1: 0.8483542976921034 certainty_f1: 0.7287575398535113 whole_f1: 0.7215829100491377\n",
      "train_Loss: 0.2194070057203961 weighted_f1: 0.7177426142291927 whole_f1: 0.7215829100491377 type_f1: 0.7107576411121799 polarity_f: 0.5831009782589764 tense_f1: 0.8483542976921034 certainty_f1: 0.7287575398535113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7c27d2b6204497af6f63a2856f5543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.20563166862357285 type_f1: 0.7452655266512851 polarity_f1: 0.6242856140843095 tense_f1: 0.8665601777901243 certainty_f1: 0.7325400931195034 whole_f1: 0.7338938554916754\n",
      "val_Loss: 0.20563166862357285 val_weighted_f1: 0.7421628529113056 val_whole_f1: 0.7338938554916754 val_type_f1: 0.7452655266512851 val_polarity_f: 0.6242856140843095 val_tense_f1: 0.8665601777901243 val_certainty_f1: 0.7325400931195034\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddc5ecd940b484eac64f2292d87fd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.2065488721780458 type_f1: 0.7354220413510728 polarity_f1: 0.6505235352129537 tense_f1: 0.8598007258218733 certainty_f1: 0.7377550932321479 whole_f1: 0.7385726955846308\n",
      "train_Loss: 0.2065488721780458 weighted_f1: 0.745875348904512 whole_f1: 0.7385726955846308 type_f1: 0.7354220413510728 polarity_f: 0.6505235352129537 tense_f1: 0.8598007258218733 certainty_f1: 0.7377550932321479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51075b9815304877bd96d6d90a7860ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1966295959986386 type_f1: 0.7992419764415659 polarity_f1: 0.6273862271909839 tense_f1: 0.8591654018701443 certainty_f1: 0.7601624720684734 whole_f1: 0.7464499036134722\n",
      "val_Loss: 0.1966295959986386 val_weighted_f1: 0.7614890193927919 val_whole_f1: 0.7464499036134722 val_type_f1: 0.7992419764415659 val_polarity_f: 0.6273862271909839 val_tense_f1: 0.8591654018701443 val_certainty_f1: 0.7601624720684734\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2816246db868457892a5fe60fb859d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.1975719420071909 type_f1: 0.7541411172788659 polarity_f1: 0.6690245674410372 tense_f1: 0.862878804069724 certainty_f1: 0.7537486047002118 whole_f1: 0.7422940022283284\n",
      "train_Loss: 0.1975719420071909 weighted_f1: 0.7599482733724596 whole_f1: 0.7422940022283284 type_f1: 0.7541411172788659 polarity_f: 0.6690245674410372 tense_f1: 0.862878804069724 certainty_f1: 0.7537486047002118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05cabf3efaf465281643721d5aa7a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19397904228858606 type_f1: 0.790305437739972 polarity_f1: 0.6369886193952611 tense_f1: 0.8591694384044835 certainty_f1: 0.7451702628047041 whole_f1: 0.7440906312488347\n",
      "val_Loss: 0.19397904228858606 val_weighted_f1: 0.7579084395861051 val_whole_f1: 0.7440906312488347 val_type_f1: 0.790305437739972 val_polarity_f: 0.6369886193952611 val_tense_f1: 0.8591694384044835 val_certainty_f1: 0.7451702628047041\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01610eb12b4f4b808868fd0cf03b7308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.18567649748646314 type_f1: 0.7784298281553019 polarity_f1: 0.6803590208717187 tense_f1: 0.8735911731122092 certainty_f1: 0.754943702397818 whole_f1: 0.7556816087571686\n",
      "train_Loss: 0.18567649748646314 weighted_f1: 0.7718309311342619 whole_f1: 0.7556816087571686 type_f1: 0.7784298281553019 polarity_f: 0.6803590208717187 tense_f1: 0.8735911731122092 certainty_f1: 0.754943702397818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae417a9e71047218cb779b6bdd023f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19156300551050867 type_f1: 0.7870020661769168 polarity_f1: 0.6600980357479721 tense_f1: 0.8721676163829802 certainty_f1: 0.758270072506334 whole_f1: 0.7525993203643297\n",
      "val_Loss: 0.19156300551050867 val_weighted_f1: 0.7693844477035507 val_whole_f1: 0.7525993203643297 val_type_f1: 0.7870020661769168 val_polarity_f: 0.6600980357479721 val_tense_f1: 0.8721676163829802 val_certainty_f1: 0.758270072506334\n",
      "val_f1 improved!! saving model...\n",
      "previous model deleted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a22f7ec82748a7a1e0e577a2f599cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.17924358938452412 type_f1: 0.7809355534555134 polarity_f1: 0.6944858697855304 tense_f1: 0.8825914784023089 certainty_f1: 0.7609365904293182 whole_f1: 0.7651524788666574\n",
      "train_Loss: 0.17924358938452412 weighted_f1: 0.7797373730181678 whole_f1: 0.7651524788666574 type_f1: 0.7809355534555134 polarity_f: 0.6944858697855304 tense_f1: 0.8825914784023089 certainty_f1: 0.7609365904293182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd6f1af649543a68c53af895aa2b5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End_Loss: 0.19056998763946778 type_f1: 0.7844224686737 polarity_f1: 0.6626117244345767 tense_f1: 0.8595981070925865 certainty_f1: 0.7596245276818907 whole_f1: 0.7511873909952069\n",
      "val_Loss: 0.19056998763946778 val_weighted_f1: 0.7665642069706885 val_whole_f1: 0.7511873909952069 val_type_f1: 0.7844224686737 val_polarity_f: 0.6626117244345767 val_tense_f1: 0.8595981070925865 val_certainty_f1: 0.7596245276818907\n",
      "val_accuracy is not improved...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3629beab2cd43b9a818fe12d0df4cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Bert_ViT_grid_search import * # 상기의 함수를 하나의 py로 저장했을 때 불러옴\n",
    "import os\n",
    "cuda_num = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cuda_num #// 초기화할 GPU number\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    \n",
    "    train_path = 'train.csv'\n",
    "    test_path = 'test.csv'\n",
    "    img_root = 'Data' # 본인이 Dacon img-text 대회에 참가했던 코드를 따왔기 때문에 무시해주세요\n",
    "    file = 'exp/my_bert' # 결과가 저장되는 위치\n",
    "    cv_k_list = [0, 1, 2, 3] # 4개로 cross validation\n",
    "    model_txt_list = ['klue_bert_base'] # 모델명\n",
    "    model_img_list = ['NaN'] # 본인이 Dacon img-text 대회에 참가했던 코드를 따왔기 때문에 무시해주세요\n",
    "    mode = 'text'  # 본인이 Dacon img-text 대회에 참가했던 코드를 따왔기 때문에 무시해주세요\n",
    "    batch_size_list = [128] # Batch size\n",
    "    lr_list = [5e-6] # LR, optimizer는 AdamW\n",
    "    max_length_list = [128] # Tokenizer의 max length\n",
    "    decay_list = [0] # optimizer의 decay\n",
    "    aug_list = [0.1] # KorEDA에서 augmentation 하는 정도 (0~1)\n",
    "    max_epoch = 15 #최대 epoch\n",
    "    interval = 1 # 저장 주기\n",
    "    train = True # False면 inference만 진행\n",
    "    initial_exp = 0 # Grid-search 초기 numbering \n",
    "    start_exp = 0 # Grid-search 시작 numbering\n",
    "       \n",
    "#     my_gs = grid_search(train_path, test_path, img_root, file, cv_k_list, model_txt_list, model_img_list, batch_size_list, lr_list, max_length_list,\n",
    "#                  decay_list, aug_list, aug_num_list, max_epoch, interval, train, initial_exp, start_exp, mode)\n",
    "    \n",
    "    my_gs = grid_search(train_path, test_path, img_root, file, cv_k_list, model_txt_list, model_img_list, batch_size_list, lr_list, max_length_list,\n",
    "             decay_list, aug_list,  max_epoch, interval, train, initial_exp, start_exp, mode)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
